{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/michieletto/hmp_utils')\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "from hmp_utils.motion.body_models_constants import SmplConstants\n",
    "from hmp_utils.motion.mocap_processor import MocapLoader\n",
    "from hmp_utils.motion.kinematics import ForwardKinematics\n",
    "from hmp_utils.visualize.stickman_animation import pose_animation, CameraOrientation, AnimationMode\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "# Definisci il percorso completo del file JSON\n",
    "percorso_file_train = '/home/michieletto/datasets/BABEL_DATASET/babel_v1.0_release/train.json'\n",
    "percorso_file_val   = '/home/michieletto/datasets/BABEL_DATASET/babel_v1.0_release/val.json'\n",
    "percorso_file_test  = '/home/michieletto/datasets/BABEL_DATASET/babel_v1.0_release/test.json'\n",
    "\n",
    "dataset.append(percorso_file_train)\n",
    "dataset.append(percorso_file_val)\n",
    "dataset.append(percorso_file_test)\n",
    "\n",
    "data = {}           # Dizionario contenente l'intero dataset  \n",
    "\n",
    "# Apre il file JSON in modalità lettura\n",
    "for i,percorso_file in enumerate(dataset):\n",
    "    with open(percorso_file, 'r') as f:\n",
    "        # Carica i dati JSON\n",
    "        data.update(json.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 ['BMLrub', 'ACCAD', 'CMU', 'MPIHDM05', 'EyesJapanDataset', 'KIT', 'EKUT', 'MPImosh', 'TCDhandMocap', 'DFaust67', 'MPILimits', 'SFU', 'TotalCapture', 'HumanEva', 'SSMsynced', 'BMLmovi', 'Transitionsmocap']\n",
      "Dati totali:  10892\n",
      "Dati con label:  5341\n",
      "Dati senza label: 5551\n",
      "num label, paths: 5341 5341\n"
     ]
    }
   ],
   "source": [
    "\n",
    "occur_lab = {\n",
    "    'label': [],              \n",
    "    'feat_p': []\n",
    "    }\n",
    "\n",
    "no_lab = 0\n",
    "\n",
    "folders=[]\n",
    "# Creazione del dataset contenente tutte le raw_label dei frame e passaggi cartelle\n",
    "for i, k in enumerate(data.keys()):\n",
    "    cont=0\n",
    "    label = data[k][\"frame_ann\"]\n",
    "    feat_p = data[k][\"feat_p\"]\n",
    "\n",
    "    parts = feat_p.split(os.sep)\n",
    "    if parts[0] not in folders:\n",
    "        folders.append(parts[0])\n",
    "\n",
    "    if label is not None:\n",
    "        for j in range(len(label[\"labels\"])):\n",
    "            raw_label = label[\"labels\"][j][\"raw_label\"]\n",
    "            if raw_label is None:\n",
    "                cont+=1\n",
    "        if cont == len(label[\"labels\"]):\n",
    "            no_lab += 1\n",
    "            continue\n",
    "\n",
    "        occur_lab[\"label\"].append(label)\n",
    "        occur_lab[\"feat_p\"].append(feat_p)\n",
    "    else:\n",
    "        no_lab += 1\n",
    "            \n",
    "print(len(folders), folders)\n",
    "print(\"Dati totali: \", len(data))\n",
    "print(\"Dati con label: \", len(occur_lab[\"label\"]))\n",
    "print(\"Dati senza label:\", no_lab)\n",
    "print(\"num label, paths:\", len(occur_lab[\"label\"]), len(occur_lab[\"feat_p\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12728\n",
      "4991\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/ACCAD'\n",
    "file_paths = glob.glob(directory + '/**/*.npz', recursive=True) # ottiene tutti i file nelle diverse cartelle contenenti i mocap in una stringa\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/KIT'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/BMLrub'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/CMU'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/MPIHDM05'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/EyesJapanDataset'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/EKUT'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/MPImosh'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/TCDhandMocap'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/DFaust67'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/MPILimits'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/SFU'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/TotalCapture'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/HumanEva'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/SSMsynced'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/BMLmovi'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "directory = '/home/michieletto/datasets/AMASS_H/Transitionsmocap'\n",
    "file_paths += glob.glob(directory + '/**/*.npz', recursive=True)\n",
    "\n",
    "file_paths = sorted(file_paths)\n",
    "print(len(file_paths))\n",
    "\n",
    "deleted_paths = []\n",
    "for i in range(len(file_paths)):\n",
    "    file_name = os.path.basename(file_paths[i])\n",
    "    folder_path = os.path.dirname(file_paths[i])   # ottiene il percorso fino alla cartella in cui è contenuto il file\n",
    "    last_folder_name = os.path.basename(folder_path)     # ottiene il nome dell'ultima cartella\n",
    "    last_folder_and_file_name = os.path.join('/', last_folder_name, file_name)     # unisce i due nomi\n",
    "    \n",
    "    if file_name == \"shape.npz\":\n",
    "        deleted_paths.append(i)\n",
    "    else:\n",
    "        for j in range(len(occur_lab[\"feat_p\"])):\n",
    "            if last_folder_and_file_name not in occur_lab[\"feat_p\"][j] and j == len(occur_lab[\"feat_p\"])-1:\n",
    "                deleted_paths.append(i)\n",
    "            elif last_folder_and_file_name in occur_lab[\"feat_p\"][j]:\n",
    "                break\n",
    "\n",
    "\n",
    "for i in sorted(deleted_paths, reverse=True):\n",
    "    del file_paths[i]\n",
    "\n",
    "print(len(file_paths))\n",
    "\n",
    "# -- load mocap --\n",
    "mocap_loader=MocapLoader(body_model_type=SmplConstants.BODY_MODEL_TYPE,\n",
    "                         keypoint_ids_to_load=SmplConstants.KEYPOINTS,\n",
    "                         target_framerate = 25)\n",
    "\n",
    "motions = []\n",
    "motions = [mocap_loader.load_mocap(path) for path in file_paths] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_vocab = []\n",
    "deleted_paths = []\n",
    "\n",
    "cont = 0\n",
    "total_frames = 75\n",
    "frames_to_jump = 10\n",
    "window_frames = 50\n",
    "for i in range(len(motions)):\n",
    "    if motions[i] is not None:\n",
    "        motions[i].rots = motions[i].rots[frames_to_jump:] # elimina i primi frame\n",
    "        if len(motions[i].rots) >= total_frames:\n",
    "            sample = []\n",
    "            valid_mocap_frames = len(motions[i].rots)\n",
    "            for j in range(int((valid_mocap_frames - 25)/window_frames)):\n",
    "                sample.append(motions[i].rots[window_frames*j:window_frames*j + total_frames])\n",
    "                assert len(sample[-1]) == 75\n",
    "\n",
    "                sample[-1] = [[angle for k_point in frame for angle in k_point] for frame in sample[-1]]\n",
    "                assert len(sample[-1][0]) == 72\n",
    "\n",
    "            frame_vocab.append(sample)\n",
    "        else:\n",
    "            frame_vocab.append([])\n",
    "            deleted_paths.append(i)\n",
    "            cont +=1\n",
    "    else:\n",
    "        frame_vocab.append([])\n",
    "        deleted_paths.append(i)\n",
    "        cont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di mocap non validi: 180\n",
      "Il numero totale di mocap è: 32455\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numero di mocap non validi: {cont}\")\n",
    "\n",
    "sum = 0\n",
    "for i in range(len(frame_vocab)):\n",
    "    sum += len(frame_vocab[i])\n",
    "    \n",
    "print(f\"Il numero totale di mocap è: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4811 4811\n",
      "Il numero di mocap del dataset è: 32455\n"
     ]
    }
   ],
   "source": [
    "dataset = {\n",
    "    'pose': [],\n",
    "    'description': [],\n",
    "}\n",
    "\n",
    "for i in range(len(file_paths)):\n",
    "    if i not in deleted_paths:\n",
    "        filepath = file_paths[i]\n",
    "        file_name = os.path.basename(filepath)    # ottiene il nome del file\n",
    "        folder_path = os.path.dirname(filepath)   # ottiene il percorso fino alla cartella in cui è contenuto il file\n",
    "        last_folder_name = os.path.basename(folder_path)     # ottiene il nome dell'ultima cartella\n",
    "        last_folder_and_file_name = os.path.join('/', last_folder_name, file_name)     # unisce i due nomi\n",
    "        \n",
    "        for j in range(len(occur_lab[\"feat_p\"])):\n",
    "            if last_folder_and_file_name in occur_lab[\"feat_p\"][j]:\n",
    "                dataset[\"pose\"].append(frame_vocab[i])\n",
    "                dataset[\"description\"].append(occur_lab[\"label\"][j])\n",
    "\n",
    "print(len(dataset['pose']), len(dataset['description']))\n",
    "\n",
    "sum = 0\n",
    "for i in range(len(dataset['pose'])):\n",
    "    sum += len(dataset['pose'][i])\n",
    "    \n",
    "print(f\"Il numero di mocap del dataset è: {sum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = []\n",
    "min_window = 5\n",
    "framerate = 25\n",
    "for i in range(len(dataset[\"description\"])):\n",
    "    frames_descriptions = []\n",
    "    labels = dataset[\"description\"][i][\"labels\"]\n",
    "    \n",
    "    # crea la lista di tutte le label annotation trovate con più di 5 frame\n",
    "    for j in range(len(labels)):\n",
    "        annotation_list = []\n",
    "        start_t = dataset[\"description\"][i][\"labels\"][j][\"start_t\"]\n",
    "        start_t = int(start_t*framerate)\n",
    "        end_t = dataset[\"description\"][i][\"labels\"][j][\"end_t\"]\n",
    "        end_t = int(end_t*framerate)\n",
    "\n",
    "        if end_t < start_t:\n",
    "            end_t, start_t = start_t, end_t\n",
    "\n",
    "        if start_t == end_t:\n",
    "            continue\n",
    "\n",
    "        annotation_list.append(start_t)\n",
    "        annotation_list.append(end_t)\n",
    "\n",
    "        raw_label = dataset[\"description\"][i][\"labels\"][j][\"raw_label\"]\n",
    "        if raw_label is not None:\n",
    "            annotation_list.append(raw_label)\n",
    "        else:\n",
    "            # non entra mai qui, ma nel dubbio lo lascio\n",
    "            annotation_list.append(\"unknown\")\n",
    "\n",
    "        frames_descriptions.append(annotation_list)\n",
    "    \n",
    "    # ordina la lista in ordeine crescente rispetto al frame di inizio\n",
    "    frames_descriptions = sorted(frames_descriptions, key=lambda x: x[0])\n",
    "\n",
    "    # se il frame più alto non corrisponde al massimo frame del mocap, lo si fa corrispondere\n",
    "    end_frames = [row[1] for row in frames_descriptions]\n",
    "    max_value = max(end_frames)\n",
    "    indices = [index for index, value in enumerate(end_frames) if value == max_value]\n",
    "    if max_value < (len(dataset[\"pose\"][i])*window_frames) + frames_to_jump + framerate:\n",
    "        for idx in indices:\n",
    "            frames_descriptions[idx][1] = (len(dataset[\"pose\"][i])*window_frames) + frames_to_jump + framerate\n",
    "\n",
    "    # corregge un paio di errori del dataset e si assicura un paio di condizioni\n",
    "    max_frame = 0\n",
    "    for l in range(len(frames_descriptions)):\n",
    "        if l > 0:\n",
    "            if frames_descriptions[l][1] == frames_descriptions[l-1][1]:\n",
    "                frames_descriptions[l-1][1] = frames_descriptions[l][0]\n",
    "\n",
    "        if max_frame < frames_descriptions[l][1]:\n",
    "            max_frame = frames_descriptions[l][1]\n",
    "\n",
    "        # assert frames_descriptions[l][0] >= frames_to_jump\n",
    "        assert frames_descriptions[l][0] < frames_descriptions[l][1]\n",
    "    \n",
    "    # creazione matrice delle label\n",
    "    matrix = [[\"\" for _ in range(max_frame)] for _ in range(len(frames_descriptions))]\n",
    "\n",
    "    for m in range(len(frames_descriptions)):\n",
    "        for col in range(frames_descriptions[m][0], frames_descriptions[m][1]):\n",
    "            matrix[m][col] = frames_descriptions[m][2]\n",
    "    \n",
    "    num_columns = len(matrix[0])\n",
    "    # Verifica quando si ha un cambio di colonna\n",
    "    flag_column = [False] * num_columns\n",
    "    \n",
    "    for col in range(1, num_columns):\n",
    "        column = [row[col] for row in matrix]\n",
    "        prev_column = [row[col-1] for row in matrix]\n",
    "        if column == prev_column:\n",
    "            flag_column[col] = False\n",
    "        else:\n",
    "            flag_column[col] = True\n",
    "    \n",
    "    flag_column[frames_to_jump] = True\n",
    "    start_frame = frames_to_jump\n",
    "    end_frame = start_frame + total_frames\n",
    "    mocap_descriptions = []\n",
    "    while (end_frame <= num_columns):\n",
    "        sentence = \"\"\n",
    "        for col in range(start_frame, end_frame):\n",
    "            if any(flag_column[col+1:col+min_window+1]) or end_frame - col <= min_window:\n",
    "                result = True\n",
    "            else:\n",
    "                result = False\n",
    "            if flag_column[col] == True and not result:\n",
    "                phrase = \"\"\n",
    "                for row in matrix:\n",
    "                    if row[col] != \"\":\n",
    "                        if phrase == \"\":\n",
    "                            phrase = row[col]\n",
    "                        else:\n",
    "                            phrase = phrase + \" and \"+ row[col]\n",
    "                if phrase == \"\":\n",
    "                    phrase = \"unknown\"\n",
    "                sentence = sentence + phrase + \" then \"\n",
    "        if end_frame < num_columns - 1:\n",
    "            flag_column[start_frame + window_frames + 1] = True\n",
    "        if sentence[-6:] == \" then \":\n",
    "            sentence = sentence[:-6]\n",
    "\n",
    "        start_frame += window_frames\n",
    "        end_frame = start_frame + total_frames\n",
    "\n",
    "        if sentence != \"\":\n",
    "            mocap_descriptions.append(sentence)\n",
    "\n",
    "    assert len(mocap_descriptions) == len(dataset[\"pose\"][i])    \n",
    "    descriptions.append(mocap_descriptions)\n",
    "\n",
    "dataset[\"description\"] = descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kine = ForwardKinematics(SmplConstants.OFFSETS, SmplConstants.PARENTS)\n",
    "kine.set_body_model_type(SmplConstants.BODY_MODEL_TYPE)\n",
    "\n",
    "for i in range(10):\n",
    "    continue\n",
    "    rand_x = random.randint(0, len(dataset[\"pose\"])-1)\n",
    "    rand_y = random.randint(0, len(dataset[\"pose\"][rand_x])-1)\n",
    "\n",
    "    print(rand_x, rand_y)\n",
    "\n",
    "    motion = dataset[\"pose\"][rand_x][rand_y]\n",
    "    motion = np.array(motion)\n",
    "    motion = motion.reshape(75, 24, 3)\n",
    "\n",
    "    keypoints_positions = kine.compute_fk(motion, apply_root_rotation=True)\n",
    "\n",
    "    print(dataset[\"description\"][rand_x][rand_y])\n",
    "\n",
    "    motions_list = [keypoints_positions]\n",
    "    subplot_ids = [0]\n",
    "    subplot_titles = [dataset[\"description\"][rand_x][rand_y]]\n",
    "    camera_orientations = [CameraOrientation(100, 20)]\n",
    "    pose_animation(motions_list, SmplConstants.PARENTS, animation_framerate = 25, subplot_ids=subplot_ids, \n",
    "                camera_orientations=camera_orientations, subplot_titles=subplot_titles,\n",
    "                fig_opts={'dpi':90}, animation_mode=AnimationMode.HTML, gif_savepath=f'./Frame_ann_gif/{dataset[\"description\"][rand_x][rand_y]}.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4811 4811\n",
      "32455 32455\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[\"description\"]), len(dataset[\"pose\"]))\n",
    "\n",
    "dataset[\"description\"] = list(itertools.chain.from_iterable(dataset[\"description\"]))\n",
    "dataset[\"pose\"] = list(itertools.chain.from_iterable(dataset[\"pose\"]))\n",
    "\n",
    "print(len(dataset[\"description\"]), len(dataset[\"pose\"]))\n",
    "\n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset_copy = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(dataset)\n",
    "dataset_copy = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michieletto/venv_pmerc/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32455,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('KennethEnevoldsen/dfm-sentence-encoder-small-v1')\n",
    "dataset[\"description\"] = model.encode(dataset[\"description\"])\n",
    "print(dataset[\"description\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m train_test_split(dataset, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mRANDOM_SEED)\n\u001b[1;32m     41\u001b[0m valid_data, test_data \u001b[38;5;241m=\u001b[39m train_test_split(test_data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mRANDOM_SEED)\n\u001b[0;32m---> 43\u001b[0m train_data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m valid_data_loader \u001b[38;5;241m=\u001b[39m get_data_loader(valid_data, batch_size)\n\u001b[1;32m     45\u001b[0m test_data_loader \u001b[38;5;241m=\u001b[39m get_data_loader(test_data, batch_size)\n",
      "Cell \u001b[0;32mIn[25], line 24\u001b[0m, in \u001b[0;36mget_data_loader\u001b[0;34m(ds, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data_loader\u001b[39m(ds, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 24\u001b[0m     class_set \u001b[38;5;241m=\u001b[39m CustomDataset(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat(), torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     26\u001b[0m     collate_fn \u001b[38;5;241m=\u001b[39m get_collate_fn()\n\u001b[1;32m     28\u001b[0m     data_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     29\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mclass_set,\n\u001b[1;32m     30\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     31\u001b[0m         collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[1;32m     32\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m     33\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensor1, tensor2):\n",
    "        self.tensor1 = tensor1\n",
    "        self.tensor2 = tensor2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor1[idx], self.tensor2[idx]\n",
    "\n",
    "def get_collate_fn():\n",
    "    def collate_fn(batch):\n",
    "        batch[\"pose\"] = [[row[i] for row in batch[\"pose\"]] for i in range(len(batch[\"pose\"][0]))]\n",
    "        # batch[\"pose\"] = np.array(batch[\"pose\"])\n",
    "        batch[\"pose\"] = torch.tensor(batch[\"pose\"])\n",
    "        batch[\"pose\"] = batch[\"pose\"].float()\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "def get_data_loader(ds, batch_size, shuffle=True):\n",
    "    pose = (torch.tensor(np.array(ds[\"pose\"]))).float()\n",
    "    desc = (torch.tensor(np.array(ds[\"description\"]))).float()\n",
    "    class_set = CustomDataset(pose, desc)\n",
    "\n",
    "    collate_fn = get_collate_fn()\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=class_set,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "batch_size = 64\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=RANDOM_SEED)\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size)\n",
    "test_data_loader = get_data_loader(test_data, batch_size)\n",
    "\n",
    "batch = next(iter(train_data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([5, 1, 4, 2, 3]), tensor([10,  6,  9,  7,  8])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensor1, tensor2):\n",
    "        self.tensor1 = tensor1\n",
    "        self.tensor2 = tensor2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor1[idx], self.tensor2[idx]\n",
    "# Creazione di un esempio di DataFrame\n",
    "data = {'col1': [1, 2, 3, 4, 5], 'col2': [6, 7, 8, 9, 10]}\n",
    "# data = pd.DataFrame(data)\n",
    "data[\"col1\"] = torch.tensor(data[\"col1\"])\n",
    "data[\"col2\"] = torch.tensor(data[\"col2\"])\n",
    "daaa = CustomDataset(data[\"col1\"], data[\"col2\"])\n",
    "print(type(data[\"col1\"]))\n",
    "data_loader = DataLoader(\n",
    "        dataset=daaa,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "next(iter(data_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pmerc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
